# -*- coding: utf-8 -*-
"""
Created on Thu Apr  2 04:53:18 2020

@author: ||MAnish Chauhan || +91-9774083186
"""

# Code for event based subscriptions.
import tableauserverclient as TSC
import pandas as pd
import smtplib   
import shutil
import os



# Defining the functions
'''********Function 1***************'''

def handle_subscriptions(pd_series_job):
    with server.auth.sign_in(tableau_auth):
           print("***************************************************************************************")
           print(" Signed In As " + uname)
           print("***************************************************************************************")
    #       j_obj=server.jobs.get(i)
    #       print(j_obj)
           while len(pd_series_job)>0:
               
               for i,v in pd_series_job.items():
                  
                   j_obj=server.jobs.get_by_id(v)
                   ##print(j_obj.finish_code)
                   if int (j_obj.finish_code)==0:
                        create_time=str(j_obj.created_at)
                        completed_time=str(j_obj.completed_at)
                        del pd_series_job[i]
    # creates SMTP session 
                        s = smtplib.SMTP('smtp.gmail.com:587') 
                        
                        s.ehlo()  
                        # start TLS for security 
                        s.starttls() 
                          
                        # Authentication 
                        s.login("sender's email","Password") 
                        subject= 'Tableau Data Source Refreshed'
                        msg='Data refresh for your tableau dashbaord was started at (UST) '+ create_time +' and finished at (UST) '+ completed_time +'.\n\n Please vist your Tableau Site for check the updated data values\n\n ** AN AUTO GENERATED MAIL' 
                        # message to be sent 
                        message = "Subject: {}\n\n{}".format(subject,msg)
                          
                        # sending the mail 
                        s.sendmail("sender's mail","receiver's email", message) 
                          
                        # terminating the session 
                        s.quit() 
                        
                     
    server.auth.sign_out               

'''***** FUCNTION 2*************'''

def move_datevents():
    for fileitem in data_events_list:
        ##print(fileitem)
        shutil.move(Path_To_Read_Events+'/'+fileitem, Move_Events_To_Path+'/'+fileitem) 

                  
print("***************************************************************************************") 
print('Functions Defined Sucessfully')
print("***************************************************************************************")

# Intializing and declaring the variables
# Mapping file should be Comma separated with .csv

Mapping_Path_Dataevent_TableauExract='C://Users/admin/Desktop/Event_Extract_map.csv' 
# Path of Data events - landing i.e. Arrival from ETL
Path_To_Read_Events='C://Users/admin/Desktop/files'
 
# Path of Data events - archival i.e. Archive post trigger of Tableau extract
Move_Events_To_Path='C://Users/admin/Desktop/filesa'

serv='tableau server address'
uname='username'
pwd='password'
site=''  # If site is Default then leave it blank else pass the site name AS ('https://mytableau.com/MYSITE')

#Capturing the number of events by listing the number of files generated by DB on refresh
data_events_list=os.listdir(Path_To_Read_Events)

# Identify unique data events
df_dataevents=pd.DataFrame(data_events_list,columns={'FileName'})
#print(df_dataevents)
df_dataevents['EventsName']= df_dataevents['FileName'].apply(lambda x: str(x)[0:str(x).rfind("_")])
#print(df_uniquedataevents)
df_uniquedataevents=pd.DataFrame(df_dataevents['EventsName'].unique(),columns={'EventName'})

 
# Import the Data event to Extract mapping
df_dataeventtoextractmapping=pd.read_csv(Mapping_Path_Dataevent_TableauExract)
 
#Identify the unique extracts to be refreshed
df_extracts=pd.merge(df_uniquedataevents,df_dataeventtoextractmapping,how='inner',left_on=['EventName'],right_on=['File_name'])

'''**********************************************************************************************************************'''

#creating the Authentication Object:tableau_auth

tableau_auth = TSC.TableauAuth(uname,pwd, site)

# Creating the Server Object:server
server = TSC.Server(serv)

#This will ignore the SSL certificate check(use this only if tableau server has SSL configured)
server.add_http_options({'verify': False}) 

# Setting Server API version i.e latest as of now
server.version = '3.1'

with server.auth.sign_in(tableau_auth):
   print("-------------------------------------------------------------------------")
   print(" Signed In As " + uname)
   print("-------------------------------------------------------------------------")

# Getting list of all data sources on tableau server by ID
   all_datasources, pagination_item = server.datasources.get()
   
#   print("\nThere are {} datasources on site: ".format(pagination_item.total_available))
   datasource_id=([datasource.id for datasource in all_datasources])
   datasource_name=([datasource.name for datasource in all_datasources])
##   print(datasource_id)
##   print(datasource_name)
# Creating dataframe with all the data source name and id's
   df_tableau_datasource=pd.DataFrame(datasource_id,index=datasource_name,columns=['ds_Id'])
##   print(df_tableau_datasource)
#  To get only those datasources which are currently available in tableau we are joing the dataframe created from events with dataframe created from all datasources vailbale in tableau
   df_extracts.set_index(['Extracts_name'],inplace=True)
   df_extracts=pd.merge(df_extracts,df_tableau_datasource,left_index= True , right_index = True) 
##   print(df_extracts)

   df_temp=(df_extracts.loc[:,'ds_Id'])
   #print(type(df_temp))    
  
    
 # Creating a list of Ids of datasources which needs to be refreshed 
   f_list=df_temp.tolist()
   ##print(f_list)
   # An empty list for storing the job ids which will generate after we hit refresh
   job=[]  
# Passing Id's of datasources listed in F_list[]  
   for i in range(len(f_list)):
       ##print(f_list[i])
       id_pass=f_list[i]  
      
# Get the data source item to update
       datasource = server.datasources.get_by_id(id_pass)      
       print(datasource)
# Call the refresh method with the data source item
       serv_refresh_obj=server.datasources.refresh(datasource)
       print("Refresh Started Sucessfully for "+f_list[i])
       print(serv_refresh_obj)
       job.append(serv_refresh_obj.id)
server.auth.sign_out
print("-------------------------------------------------------------------------")
print("Refresh for "+str(len(f_list))+' datasources have been sucessfully intiated')       
print("-------------------------------------------------------------------------")
# Move Data event files to Archive Folder
move_datevents()
# Puting the Job ids of all extracts in a pandas series which is used in getting the status of job an then fire the email from handling_Subscription fucntion
pd_series_job=pd.Series(job)
handle_subscriptions(pd_series_job)


print('\\\***********************MAIL ALERT SENT SUCESSFULLY***************************************')  
